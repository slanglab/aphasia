{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b582c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "\n",
    "def get_response_llama3(prompt, temperature=0.67, max_tokens=700):\n",
    "    \n",
    "    endpoint = 'https://api.together.xyz/v1/chat/completions'\n",
    "    res = requests.post(endpoint, json={\n",
    "        \"model\": \"meta-llama/Llama-3-70b-chat-hf\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.7,\n",
    "        \"top_k\": 50,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"stop\": [\n",
    "            \"<|eot_id|>\"\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": prompt,\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "        ]\n",
    "    }, headers={\n",
    "        \"Authorization\": 'api-key',\n",
    "    })  \n",
    "    \n",
    "    return res.json()['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data \n",
    "\n",
    "def save_jsonl(name, data):\n",
    "    with open(name, 'w') as outfile:\n",
    "        for entry in data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e76e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_stimuli = ['MarcusYam', 'SylviaEarle', 'NaomiDeLaRosa', 'RobinSteinberg']\n",
    "sd_stimuli = ['AuntMother', 'Ferguson', 'Sept11', 'NoHandbook']\n",
    "list_of_stimuli = sd_stimuli+vs_stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743fedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_filenames = os.listdir('./Prompts/zero_shot/')\n",
    "prompt_dict = {}\n",
    "for p in prompt_filenames:\n",
    "    if '.txt' not in p:\n",
    "        continue\n",
    "    with open('./Prompts/zero_shot/'+p, 'r') as f:\n",
    "        prompt = f.read()\n",
    "        \n",
    "    prompt_dict[p.split('.')[0]] = prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2acde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_narrative_dict = {}\n",
    "for stimulus in list_of_stimuli:\n",
    "    with open('../data/BATS/originalstories/'+stimulus+'.txt', 'r') as f:\n",
    "        narrative = f.read()\n",
    "    original_narrative_dict[stimulus] = narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e546bd",
   "metadata": {},
   "source": [
    "## generate MCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './Responses/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'\n",
    "temperature = 0.67\n",
    "\n",
    "for sample_num in range(5):\n",
    "    for prompt_name, template in prompt_dict.items():\n",
    "\n",
    "        save_data = []\n",
    "        for idx, stimulus in tqdm(enumerate(list_of_stimuli)):\n",
    "\n",
    "            original_narrative = original_narrative_dict[stimulus]\n",
    "            prompt = template.format(original_narrative)\n",
    "            \n",
    "            metadata = {}\n",
    "            metadata['Prompt'] = prompt\n",
    "            metadata['Stimuli'] = stimulus\n",
    "            metadata['original_narrative'] = original_narrative\n",
    "            metadata['row'] = idx\n",
    "\n",
    "            response = get_response_llama3(prompt, temperature=temperature, max_tokens=700)\n",
    "            metadata['response'] = response\n",
    "            save_data.append(metadata)\n",
    "            save_jsonl(save_dir+prompt_name+'_temp_'+str(temperature)+'_'+str(sample_num)+'.jsonl', save_data)\n",
    "            time.sleep(2)\n",
    "        save_jsonl(save_dir+prompt_name+'_temp_'+str(temperature)+'_'+str(sample_num)+'.jsonl', save_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc85608b",
   "metadata": {},
   "source": [
    "## decompose MCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b04e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os, re\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "endpoint = 'https://api.together.xyz/v1/chat/completions'\n",
    "model = \"meta-llama/Llama-3-70b-chat-hf\" \n",
    "\n",
    "def construct_clean_up_prompt(text):\n",
    "    instruction = '''Please format the given text into plain text format, without any formatting, numbers or headings (e.g., 'Tragic Event', \"Thumb\") or introductory text (e.g., Here is the list). Do not reword the text. Only return a list and nothing else (e.g., phrases like 'Here is the list of main concepts in plain text format:').'''\n",
    "    instruction+='\\n\\nText: \\n\\n'+text\n",
    "    return instruction\n",
    "            \n",
    "            \n",
    "with open('./Prompts/decomposition_decontextualization.txt', 'r') as f:\n",
    "    decomposition_prompt = f.read()\n",
    "    \n",
    "\n",
    "input_dir = './Responses/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'\n",
    "response_files = os.listdir(input_dir)\n",
    "response_files = [r for r in response_files if '.jsonl' in r]\n",
    "\n",
    "for r in tqdm(response_files):\n",
    "    data = load_jsonl(input_dir+r)\n",
    "    \n",
    "    if 'decomposed_mcs' in data[-1]:\n",
    "        continue\n",
    "    \n",
    "    for d in tqdm(data):\n",
    "        if 'decomposed_mcs' in d:\n",
    "            continue\n",
    "        raw_response = d['response']\n",
    "        \n",
    "        mc_list = get_response_llama3(construct_clean_up_prompt(raw_response), temperature=0.0, max_tokens=1000).split('\\n')\n",
    "        time.sleep(2)\n",
    "        \n",
    "        decomposed_mcs = []\n",
    "        for m in mc_list:\n",
    "            if m=='': continue\n",
    "            atomic_mcs = get_response_llama3(decomposition_prompt.format(m), temperature=0.0, max_tokens=512).split('\\n')\n",
    "            atomic_mcs = [a.strip('-').strip(' ') for a in atomic_mcs]\n",
    "            decomposed_mcs+=atomic_mcs\n",
    "            time.sleep(10)\n",
    "        \n",
    "            \n",
    "        d['decomposed_mcs'] = decomposed_mcs\n",
    "        save_jsonl(input_dir+r, data)\n",
    "    \n",
    "        time.sleep(40)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd102269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Prompt', 'Stimuli', 'original_narrative', 'row', 'response', 'decomposed_mcs'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = './Responses/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'\n",
    "response_files = os.listdir(input_dir)\n",
    "response_files = [r for r in response_files if '.jsonl' in r]\n",
    "\n",
    "data = load_jsonl(input_dir+response_files[0])\n",
    "\n",
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8a023",
   "metadata": {},
   "source": [
    "### analysis of decomposed MCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89d1a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_complexity(bullet_points: list) -> float:\n",
    "    ''' \n",
    "    Complexity: the average number of words per bullet point.\n",
    "\n",
    "    Args:\n",
    "      bullet_points: list of strings, each string is a bullet point/main concept. Returned from get_yield() with return_sents=True\n",
    "    '''\n",
    "\n",
    "    total_words = 0\n",
    "    for sent in bullet_points:\n",
    "        total_words += len(sent.split()) # split by whitespace\n",
    "\n",
    "    return np.round(total_words / len(bullet_points), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "121ee722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 1151.20it/s]\n"
     ]
    }
   ],
   "source": [
    "complexity_data = []\n",
    "for r in tqdm(response_files):\n",
    "    data = load_jsonl('./Responses/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'+r)\n",
    "    for d in data:\n",
    "        stimuli = d['Stimuli']\n",
    "        decomposed_mcs = d['decomposed_mcs']\n",
    "        complexity = get_complexity(decomposed_mcs)\n",
    "        d['decomposed_mc_complexity'] = complexity\n",
    "        complexity_data.append({'Stimuli':stimuli, 'prompt': r.split('.')[0], 'complexity': complexity})\n",
    "        \n",
    "complexity_data = pd.DataFrame(complexity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40770a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Complexity of generated MCs======\n",
      "AuntMother 7.75\n",
      "Ferguson 9.3\n",
      "Sept11 8.48\n",
      "NoHandbook 9.19\n",
      "MarcusYam 8.82\n",
      "SylviaEarle 9.03\n",
      "NaomiDeLaRosa 7.77\n",
      "RobinSteinberg 8.93\n"
     ]
    }
   ],
   "source": [
    "print('======Complexity of generated MCs======')\n",
    "for s in list_of_stimuli:\n",
    "    m = complexity_data[complexity_data.Stimuli==s]['complexity'].mean()\n",
    "    print(s, np.round(m, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58da454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Complexity of gold MCs======\n",
      "AuntMother 8.71\n",
      "Ferguson 8.8\n",
      "Sept11 7.0\n",
      "NoHandbook 8.64\n",
      "MarcusYam 7.18\n",
      "SylviaEarle 8.75\n",
      "NaomiDeLaRosa 8.12\n",
      "RobinSteinberg 7.57\n"
     ]
    }
   ],
   "source": [
    "def get_main_concepts(stimuli_name: str) -> dict:\n",
    "    with open('../data/BATS/goldMCs/' + stimuli_name + '.json', 'r') as f:\n",
    "        concepts = json.load(f)\n",
    "    return concepts\n",
    "\n",
    "print('======Complexity of gold MCs======')\n",
    "for s in list_of_stimuli:\n",
    "    print(s, get_complexity(list(get_main_concepts(s).values())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
