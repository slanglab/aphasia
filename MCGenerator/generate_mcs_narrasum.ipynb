{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5564e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "def num_tokens_from_string(string, encoding):\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data \n",
    "\n",
    "def save_jsonl(name, data):\n",
    "    with open(name, 'w') as outfile:\n",
    "        for entry in data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "def get_yield(sentences):\n",
    "    num_toks = 0\n",
    "    for s in sentences:\n",
    "        num_toks+=len(s.split())\n",
    "    return num_toks\n",
    "\n",
    "def get_complexity(bullet_points: list) -> float:\n",
    "    ''' \n",
    "    Complexity: the average number of words per bullet point.\n",
    "\n",
    "    Args:\n",
    "      bullet_points: list of strings, each string is a bullet point/main concept. Returned from get_yield() with return_sents=True\n",
    "    '''\n",
    "\n",
    "    total_words = 0\n",
    "    for sent in bullet_points:\n",
    "        total_words += len(sent.split()) # split by whitespace\n",
    "\n",
    "    return np.round(total_words / len(bullet_points), 2)\n",
    "\n",
    "def get_response_llama3(prompt, temperature=0.67, max_tokens=700):\n",
    "    \n",
    "    endpoint = 'https://api.together.xyz/v1/chat/completions'\n",
    "    res = requests.post(endpoint, json={\n",
    "        \"model\": \"meta-llama/Llama-3-70b-chat-hf\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.7,\n",
    "        \"top_k\": 50,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"stop\": [\n",
    "            \"<|eot_id|>\"\n",
    "        ],\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": prompt,\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "        ]\n",
    "    }, headers={\n",
    "        \"Authorization\": 'api-key',\n",
    "    })  \n",
    "    \n",
    "    return res.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e04ec19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_filenames = os.listdir('./Prompts/zero_shot/')\n",
    "prompt_dict = {}\n",
    "for p in prompt_filenames:\n",
    "    if 'ipynb_checkpoints' in p: continue\n",
    "    with open('./Prompts/zero_shot/'+p, 'r') as f:\n",
    "        prompt = f.read()\n",
    "        \n",
    "    prompt_dict[p.split('.')[0]] = prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1a7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_jsonl('../data/narrasum/narrasum_sampled_data.jsonl')\n",
    "\n",
    "dataset = {}\n",
    "for idx, d in enumerate(data):\n",
    "    sentences = sent_tokenize(d['summary'])\n",
    "    dataset[idx] = {\n",
    "                    'summary': d['summary'],\\\n",
    "                    'document': d['document'],\\\n",
    "                    'summary_sentences': sentences,\\\n",
    "                    'num_summary_sentences': len(sentences),\\\n",
    "                    'complexity_summary_sentences': get_complexity(sentences),\\\n",
    "                    'yield_summary_sentences': get_yield(sentences),\\\n",
    "                    'yield_document_sentences': get_yield(sent_tokenize(d['document']))\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = './Responses/narrasum/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'\n",
    "temperature = 0.67\n",
    "\n",
    "for sample_num in [0,1,2,3,4]:\n",
    "    for prompt_name, template in prompt_dict.items():\n",
    "        save_file_name = save_dir+prompt_name+'_temp_'+str(temperature)+'_'+str(sample_num)+'.jsonl'\n",
    "       \n",
    "        # Initialize save_data\n",
    "        save_data = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        # Check if file exists and load existing data\n",
    "        if os.path.exists(save_file_name):\n",
    "            save_data = load_jsonl(save_file_name)\n",
    "            if len(save_data) == len(dataset):  # If complete, skip to next file\n",
    "                print(f\"Skipping completed file: {save_file_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Find the last processed index\n",
    "            if save_data:\n",
    "                last_processed = max(d['row'] for d in save_data)\n",
    "                start_idx = last_processed + 1\n",
    "                print(f\"Resuming from index {start_idx} for {save_file_name}\")\n",
    "            \n",
    "        for idx in tqdm(range(start_idx, len(dataset)), initial=start_idx, total=len(dataset)):\n",
    "\n",
    "            original_narrative = dataset[idx]['document']\n",
    "            prompt = template.format(original_narrative)\n",
    "\n",
    "            metadata = {}\n",
    "            metadata['Prompt'] = prompt\n",
    "            metadata['original_narrative'] = original_narrative\n",
    "            metadata['row'] = idx\n",
    "\n",
    "            response = get_response_llama3(prompt, temperature=temperature, max_tokens=2048)\n",
    "            metadata['response'] = response\n",
    "            save_data.append(metadata)\n",
    "            \n",
    "            save_jsonl(save_dir+prompt_name+'_temp_'+str(temperature)+'_'+str(sample_num)+'.jsonl', save_data)\n",
    "            \n",
    "            time.sleep(2)\n",
    "            \n",
    "            if idx==50:\n",
    "                time.sleep(10)\n",
    "            \n",
    "        save_jsonl(save_dir+prompt_name+'_temp_'+str(temperature)+'_'+str(sample_num)+'.jsonl', save_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fcfb7a",
   "metadata": {},
   "source": [
    "### decompose mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os, re\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "endpoint = 'https://api.together.xyz/v1/chat/completions'\n",
    "model = \"meta-llama/Llama-3-70b-chat-hf\" \n",
    "\n",
    "\n",
    "def construct_clean_up_prompt(text):\n",
    "    instruction = '''Please format the given text into plain text format, without any formatting, numbers or headings (e.g., 'Tragic Event', \"Thumb\") or introductory text (e.g., Here is the list). Do not reword the text. Only return a list and nothing else (e.g., phrases like 'Here is the list of main concepts in plain text format:').'''\n",
    "    instruction+='\\n\\nText: \\n\\n'+text\n",
    "    return instruction\n",
    "            \n",
    "            \n",
    "with open('./Prompts/decomposition_decontextualization.txt', 'r') as f:\n",
    "    decomposition_prompt = f.read()\n",
    "    \n",
    "    \n",
    "### Run decomposer for generated MCs ####\n",
    "input_dir = './Responses/narrasum/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'\n",
    "response_files = os.listdir(input_dir)\n",
    "response_files = [r for r in response_files if '.jsonl' in r]\n",
    "\n",
    "\n",
    "for r in tqdm(response_files):\n",
    "    data = load_jsonl(input_dir+r)\n",
    "    \n",
    "    if 'decomposed_mcs' in data[-1]:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    for d in tqdm(data):\n",
    "        if 'decomposed_mcs' in d:\n",
    "            continue\n",
    "        raw_response = d['response']\n",
    "        \n",
    "        mc_list = get_response_llama3(construct_clean_up_prompt(raw_response), temperature=0, max_tokens=1000).split('\\n')\n",
    "        time.sleep(2)\n",
    "        \n",
    "        decomposed_mcs = []\n",
    "        for m in mc_list:\n",
    "            if m=='': continue\n",
    "            atomic_mcs = get_response_llama3(decomposition_prompt.format(m), 512).split('\\n')\n",
    "            atomic_mcs = [a.strip('-').strip(' ') for a in atomic_mcs]\n",
    "            decomposed_mcs+=atomic_mcs\n",
    "            time.sleep(10)\n",
    "        \n",
    "            \n",
    "        d['decomposed_mcs'] = decomposed_mcs\n",
    "        save_jsonl(input_dir+r, data)\n",
    "    \n",
    "        time.sleep(40)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3d34e",
   "metadata": {},
   "source": [
    "### Decomposer for gold MCs (this step is needed as the original dataset has free text summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data = load_jsonl('../data/narrasum/narrasum_sampled_data.jsonl')\n",
    "\n",
    "for d in tqdm(data):\n",
    "    sentences = sent_tokenize(d['summary'])\n",
    "    \n",
    "    decomposed_mcs = []\n",
    "    for m in sentences:\n",
    "        if m=='': continue\n",
    "        atomic_mcs = get_response_gpt4(decomposition_prompt.format(m), max_tokens).split('\\n')\n",
    "        \n",
    "        atomic_mcs = [a.strip('-').strip(' ') for a in atomic_mcs]\n",
    "        \n",
    "        decomposed_mcs+=atomic_mcs\n",
    "        time.sleep(1)  \n",
    "        \n",
    "\n",
    "    d['decomposed_summary_sentences_gpt4'] = decomposed_mcs\n",
    "    \n",
    "save_jsonl('../data/narrasum/narrasum_sampled_data_with_decomposed_mcs_gpt4.jsonl', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
