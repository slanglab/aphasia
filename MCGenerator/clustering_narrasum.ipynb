{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22138b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, re\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b27b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data \n",
    "\n",
    "def save_jsonl(name, data):\n",
    "    with open(name, 'w') as outfile:\n",
    "        for entry in data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea7d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrasum_dataset = load_jsonl('../data/narrasum/narrasum_sampled_data_with_decomposed_mcs_gpt4.jsonl')\n",
    "\n",
    "gold_concepts = {}\n",
    "for idx, d in enumerate(narrasum_dataset):\n",
    "    gold_concepts[idx] = d['decomposed_summary_sentences_gpt4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236c17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_concepts_dir = '../MCGenerator/Responses/narrasum/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'\n",
    "eval_response_save_dir = './Responses/narrasum/zero_shot/llama3/original_narrative_input/monte-carlo-same-temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81baa027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five_finger_definition_2_temp_0.67_0.jsonl\n",
      "five_finger_definition_2_temp_0.67_1.jsonl\n",
      "five_finger_definition_2_temp_0.67_2.jsonl\n",
      "five_finger_definition_2_temp_0.67_3.jsonl\n",
      "five_finger_definition_2_temp_0.67_4.jsonl\n",
      "five_ws_definition_temp_0.67_0.jsonl\n",
      "five_ws_definition_temp_0.67_1.jsonl\n",
      "five_ws_definition_temp_0.67_2.jsonl\n",
      "five_ws_definition_temp_0.67_3.jsonl\n",
      "five_ws_definition_temp_0.67_4.jsonl\n",
      "help_me_understand_temp_0.67_0.jsonl\n",
      "help_me_understand_temp_0.67_1.jsonl\n",
      "help_me_understand_temp_0.67_2.jsonl\n",
      "help_me_understand_temp_0.67_3.jsonl\n",
      "help_me_understand_temp_0.67_4.jsonl\n"
     ]
    }
   ],
   "source": [
    "target_prompts = ['help_me_understand', 'five_ws_definition', 'five_finger_definition_2']\n",
    "prompt_names = os.listdir(generated_concepts_dir)\n",
    "filtered_prompts = [name for name in prompt_names if any(target in name for target in target_prompts)]\n",
    "\n",
    "stimuli2mcs = {}\n",
    "for prompt_name in sorted(filtered_prompts):\n",
    "    if '.ipynb_checkpoints' in prompt_name: continue\n",
    "        \n",
    "    print(prompt_name)\n",
    "        \n",
    "    data = load_jsonl(generated_concepts_dir+'/'+prompt_name.split('/')[-1])\n",
    "    \n",
    "    for r in data:\n",
    "        if r['row'] not in stimuli2mcs:\n",
    "            stimuli2mcs[r['row']] = r['decomposed_mcs']\n",
    "        else:\n",
    "            stimuli2mcs[r['row']].extend(r['decomposed_mcs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410d6ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitagupta/anaconda3/envs/supcourt/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/ankitagupta/anaconda3/envs/supcourt/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ankitagupta/anaconda3/envs/supcourt/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from pdc_dp_means import DPMeans\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Compute the cosine similarity between two vectors.\"\"\"\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / (norm_v1 * norm_v2)\n",
    "\n",
    "def find_closest_examples(n_clusters, cluster_centers, embeddings, cluster_predictions, mcs):\n",
    "    closest_examples = {}\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        if cluster not in cluster_predictions: continue\n",
    "        cluster_indices = np.where(cluster_predictions == cluster)[0]\n",
    "        cluster_embeddings = embeddings[cluster_indices]\n",
    "        \n",
    "        similarities = [\n",
    "            cosine_similarity(embedding, cluster_centers[cluster]) \n",
    "            for embedding in cluster_embeddings\n",
    "        ]\n",
    "        closest_index_in_cluster = np.argmax(similarities)\n",
    "        \n",
    "        # distances = np.linalg.norm(cluster_embeddings - cluster_centers[cluster], axis=1)\n",
    "        # closest_index_in_cluster = np.argmin(distances)\n",
    "\n",
    "        closest_examples[cluster] = mcs[cluster_indices[closest_index_in_cluster]]\n",
    "\n",
    "    return closest_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "from pdc_dp_means import DPMeans\n",
    "\n",
    "deltas = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4]\n",
    "\n",
    "mcs_per_stimuli = {}\n",
    "for stimuli in tqdm(np.arange(100)):\n",
    "    \n",
    "    mcs = stimuli2mcs[stimuli]\n",
    "    mcs = [m for m in mcs if m!='']\n",
    "    \n",
    "    mc_embeddings = model.encode(mcs)\n",
    "    mc_embeddings = mc_embeddings.astype(np.float64)\n",
    "    mc_embeddings = normalize(mc_embeddings, norm='l2')\n",
    "    assert mc_embeddings.shape[0]==len(mcs), 'error in embedding process'\n",
    "    \n",
    "    mcs_per_delta = {}\n",
    "    for delta in deltas:\n",
    "        #print(delta)\n",
    "        dpmeans = DPMeans(n_init=1, delta=delta)  # n_init and delta parameters\n",
    "        dpmeans.fit(mc_embeddings)\n",
    "\n",
    "        cluster_predictions = dpmeans.predict(mc_embeddings)\n",
    "        cluster_centers = dpmeans.cluster_centers_\n",
    "        n_clusters = dpmeans.n_clusters\n",
    "        closest_examples = find_closest_examples(n_clusters, cluster_centers,\\\n",
    "                                                 mc_embeddings,\\\n",
    "                                                 cluster_predictions,\\\n",
    "                                                 mcs)\n",
    "\n",
    "        mcs_per_delta[delta] = {'closest_examples': list(closest_examples.values()),\n",
    "                                  'n_clusters': n_clusters,\\\n",
    "                                  'cluster_predictions': cluster_predictions,\\\n",
    "                                  'cluster_centers': cluster_centers}\n",
    "    \n",
    "    mcs_per_stimuli[stimuli] = mcs_per_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1dc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# with open('./clustering_results/narrasum/results/clustered_mcs_topthreeprompts_run2.pickle', 'wb') as f:\n",
    "#     pickle.dump(mcs_per_stimuli, f)\n",
    "    \n",
    "with open('./clustering_results/narrasum/results/clustered_mcs_topthreeprompts_run2.pickle', 'wb') as f:\n",
    "    pickle.dump(mcs_per_stimuli, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
